## 一、AI时代的研发变革

### 1.1 从人力向算力的转型

传统软件研发的生产要素由人力、技术、信息组成，核心逻辑是"多招人→多做项目→更多产出"。但这种模式围绕"人"这一要素存在诸多挑战：好人才难招、新人需培养、人才会流失、个体动性差异、协作效率受限。

AI时代，研发的核心要素正在发生根本性转变：

- **算力成为新的增长引擎**：增长逻辑转变为"多买显卡→获取更多算力→更多产出"
- **工作向算力迁移**：大部分体力工作向算力迁移，部分脑力工作可沉淀到算力中
- **人的角色转型**：人从直接产出转为对算力的设计与编排

这一转型在硅谷大厂已有明显体现：一边裁员，一边争相购买显卡，标志着"马拉火车"时代即将过去，算力终将超越人力。

### 1.2 企业级软件工程的四大特点

AI Coding落地需要解决的关键问题可归纳为：

1. **复杂度高**：企业级项目涉及多系统、多技术栈、复杂业务逻辑
2. **规范性强**：需遵循企业内部的编码规范、架构标准、安全要求
3. **协同要求高**：涉及多角色协作，需要统一的工作流和沟通机制
4. **质量要求严格**：对稳定性、可维护性、安全性有更高要求

## 二、AI落地的三个阶段

![](https://cdn.gooo.ai/web-images/6ea0772442dcf842cb0cdac7b175f101720e29d1c08b394c63e5c06319ed360b)

有赞将AI落地实践划分为三个递进阶段：

### 阶段一：AI辅助

- **特征**：AI作为工具辅助人工完成任务
- **应用场景**：代码补全、智能建议、文档生成
- **人机关系**：人主导，AI提供支持

### 阶段二：AI增强

- **特征**：AI承担部分任务，人进行监督和决策
- **应用场景**：自动化代码生成、测试用例生成、代码审查
- **人机关系**：AI与人协作，各司其职

### 阶段三：AI驱动

- **特征**：AI自主完成大部分任务，人进行高层决策和验证
- **应用场景**：完整需求交付、端到端自动化
- **人机关系**：AI主导执行，人监督结果

**关键认知**：AI的落地无法一蹴而就，需要循序渐进。不同场景应匹配适合的阶段，过度追求AI反而适得其反。目前有赞的实践大量在AI增强阶段，少量能到AI驱动阶段。

## 三、AI Coding：从个人助手到规模协同

### 3.1 设计路线的选择

在AI Coding的落地中，有两条可选路线：

**路线一：以人为主，Agent为辅**

- 优势：可控性强，易于理解
- 劣势：上下文切换频繁、缺乏系统化沉淀、个人经验难复用

**路线二：以Agent为主，人监督**

- 优势：系统化解决问题、经验可沉淀复用、"天花板"更高
- 劣势：初期投入大，需要完善的基础设施

有赞选择了第二条路线，将开发模式类比为"技术Leader带领小团队"：由Leader负责任务拆分、方案设计、过程把控和代码CR，而多个AI Agent承担具体的编码工作。

### 3.2 构建框架：八大核心模块

![](https://cdn.gooo.ai/web-images/6ab74f4e04384b2c3e8cb3771c3a7e50b104674d28ccb22a383ee09198077095)

#### 3.2.1 Base Agent与System Prompt

**选型策略**：将通识的交给行业，集中资源做私有部分，通过行业迭代来提升底层能力。

- **基础Agent选择**：Claude Code（而非单纯的Claude）
- **选择理由**：在子Agent调度、MCP集成、通用开发工具、上下文压缩方面已有成熟实践
- **系统提示词**：仅做少量扩展（不到200行），主要包括内部编程规范、特定约束

#### 3.2.2 Dev Tools（开发工具集成）

通过**MCP（Model Context Protocol）**将分散在各个系统的工具集成起来：

- 需求管理平台查询
- 飞书文档创建与编辑
- 代码仓库操作
- 内部API调用
- 技术方案评审

Agent可根据任务需求自主决策使用哪些工具，实现与企业内部研发体系的深度连接。

#### 3.2.3 Codebase Search（代码理解能力）

代码理解能力分为三层：

**第一层：目标仓库定位**

- 方案：知识库RAG
- 目的：快速定位需要操作的代码仓库

**第二层：跨仓库依赖代码召回**

- 方案：RAG + Git增量更新
- 特点：基于Git提交记录进行增量更新，平衡实时性和性能
- 安全：所有代码片段通过安全扫描进行脱敏

**第三层：工作区代码理解**

- 方案：文本搜索（继承Claude Code能力）
- 特点：精度高、实时性强

**技术对比**：

- Cursor选择RAG方案：速度快，适合大单体仓库，但精度有损失
- Claude Code选择文本搜索：精度高、实时性强，但性能稍差
- 有赞采用混合方案：根据场景特点选择最优技术组合

#### 3.2.4 Knowledge Base（知识库建设）

AI Agent需要两类知识：

**业务知识**：

- 产品模块结构
- 业务术语定义
- 领域规则

**专业知识**：

- 技术选型规范
- 编程规范
- 工程现状

**知识库建设的两大痛点**：

1. 信息孤岛：无法统一检索，重复建设严重
2. 内容老旧：缺少审核/运营机制，大量过期/错误内容

**解决方案**：成立专门的知识工程团队，围绕知识增长、内容质量、知识使用进行建设，为上层应用提供高质量的知识。

#### 3.2.5 Long-term Memory（长期记忆学习系统）

**核心问题**：大量经验在开发者头脑中，无法直接传递给Agent。

**解决方案**：基于长期记忆的学习系统

**工作流程**：

1. **记忆提取**：将开发者与Agent的监督对话提取为长期记忆
2. **记忆存储**：结构化存储，建立索引
3. **记忆召回**：遇到类似场景时召回使用
4. **跨会话复用**：实现知识的持续积累

**效果提升**：

- 未接入记忆：需要5-10轮对话修正
- 接入记忆：仅需1-3轮对话

**加速技巧**：通过人工标注及修正，可快速提升记忆的可用性。

#### 3.2.6 Cloud Sandbox（云端沙箱）

**为什么选择云端而非本地？**

本地运行存在的问题：

- 复杂环境导致额外上下文
- 工程化适配问题
- 安全与监管难解决

**云端沙箱设计**：

- 每个Agent的每次会话分配独立且标准化的沙箱
- 包含完整开发环境，实现交付结果预览
- 会话与沙箱无状态化，实现水平扩容
- 会话存储在共享存储中，支持动态恢复
- 统一网关管理，解决安全和监管问题

#### 3.2.7 Multi Agents System（多Agent系统）

**引入原因**：单一Agent面对多环节时，因LLM上下文长度导致遗忘、性能问题。

**解决方案**：将复杂度分而治之

**差异化设计**：

- 根据Agent任务特点选择不同的基础模型
- 考虑LLM的优劣势与成本
- 实现专业化分工

#### 3.2.8 HITL（人工监督）与发布系统对接

**人工监督系统**：

- 面向开发者：对话明细、实时反馈、问题修正
- 面向管理者：效能分析、质量监控、成本统计

**发布系统对接**：

- 通过部署发布Agent打通发布系统
- 支持对话式部署到预发环境
- 支持对话式发布到生产环境

### 3.3 产品形态与需求选择

**当前产品形态**：以对话式交互为主，正在开发类似manus的Web端界面。

**需求选择矩阵**：

![](https://cdn.gooo.ai/web-images/c94c24fc53e8e5674025893ef9b5f4ff373ad2dc6b3d49515587af2e84fddfe2)

分为三个维度：

- **职能维度**：单职能 → 多职能
- **仓库维度**：单仓库 → 多仓库
- **规模维度**：日常需求 → 项目级需求

**落地策略**：

1. 从单职能单仓库的日常需求开始
2. 通过小需求验证Agent并积累数据
3. 逐步向多职能多仓库、项目级需求迭代

**重要认知**：AI不是万能的，人做不了的它也做不了。需要合理设定期望。

### 3.4 落地效果数据

**当前能力**：

- 可实现单职能多仓库的日常需求
- 已交付近百个需求

**效能数据**：

- 综合提效：30%（包括人工监督耗时）
- 单个需求Token费用：不到100元人民币

**下一步方向**：

- 多职能多仓库需求
- 项目级需求

### 3.5 典型应用场景

#### 3.5.1 翻译型任务

**特征**：已有明确方案且较为简单的任务

**案例**：基础库升级

- 涉及：1个基础库 + 若干业务库 + 23个业务应用
- 传统方式：需要超过50人日
- AI完成：几十分钟
- 质量：达到生产标准

#### 3.5.2 跨域编程

**特征**：需要跨团队、跨领域支援的任务

**价值**：

- 传统方式：学习和熟悉过程需要额外时间
- AI方式：几乎抹平学习成本
- 效果：开发者可顺畅完成非本领域的编程任务

#### 3.5.3 移动编程场景

**案例**：开发者在聚餐未带电脑时，通过手机完成Bug修复

- 接到Bug通知
- 通过手机与AI Agent对话
- AI完成代码修复
- 通过手机审查并部署

**意义**：AI正在改变编程的时空限制

## 四、AI Test：从自动化到智能化

### 4.1 传统测试的局限与AI时代的新挑战

**传统测试面临的挑战**：

- 技术栈多样化
- 设备终端碎片化
- 工程规模增大

**自动化测试的局限**：

- 编写有门槛
- 维护成本高
- 难扩展复用
- 失败排查困难

**AI时代的新挑战**：

- 编码效率提升，对测试效率要求更高
- AI生成代码不确定性大，影响面和风险更大

**AI带来的新可能**：

- 测试用例生成与维护
- 测试执行与分析
- 缺陷预测与定位

### 4.2 AI用例标准化

**核心问题**：历史用例质量参差不齐，导致GIGO（Garbage in, Garbage out）现象。

**用例问题**：

- 隐式步骤过多
- 断言缺失
- 分散在各个平台
- 文本用例和自动化用例互不相通

**解决方案：自然语言用例**

利用LLM强大的自然语言理解能力，让用例兼具：

- **语意性**：人类易读易理解
- **可执行性**：可直接用于自动化测试

**两个方向**：

**存量用例优化**：

- 由LLM生成更规范的用例名、标签
- 逐步优化用例步骤
- 补充断言

**增量用例生成**：

- 结合业务知识库
- 参考现有用例
- 自动生成新用例

**用例生成三步流程**：

1. 根据测试目标和上下文生成初步用例
2. 结合业务知识进行完善
3. 参考相似用例进行优化

**当前状态**：用例生成的准确度和覆盖场景仍有较大提升空间，处于探索阶段。

### 4.3 AI增强录制

**传统录制的局限**：

- 定位不精确
- 录制步骤冗余
- 需要人工校准
- 维护成本高

**AI增强方案**：

**核心流程**：

1. **并行录制**：人操作时，同步录制图片+坐标
2. **AI分析**：多模态LLM分析生成自然语言步骤（约5秒）
3. **步骤优化**：AI识别用户意图，生成精炼步骤
4. **智能回放**：AI理解步骤语义，自适应执行

**技术演进**：

- 截图与坐标记录
- 多模态模型分析（图片→文本步骤）
- 步骤合并与优化
- 意图识别与语义理解
- 自适应执行引擎

**关键技术点**：

- 将多次操作合并为单一意图
- 精确识别操作目标（如"输入价格1"、"输入划线价2"）
- 屏蔽无关操作

**性能数据**：

- 准确率：89%
- 单次增强耗时：5-10秒
- 模型演进：Qwen2.5-VL（20秒）→ GPT-4o（10秒内）

**跨平台支持**：

- Web、Pad、桌面等设备
- Android、iOS、鸿蒙等系统
- 各类小程序

### 4.4 AI用例执行

**架构设计**：

- 统一任务中心：所有用例执行注册到任务中心
- 两大集群：浏览器任务集群、App任务集群（含小程序）

**AI执行的优势**：

- **跨平台支持**：一套用例适配多平台
- **自适应能力**：处理像素抖动等不确定性
- **稳定性提升**：LLM的理解能力提升用例鲁棒性

**执行数据**：

- 日均执行量：超过10万次
- 任务成功率：96%

**遇到的问题**：

- 模型执行速度慢
- 模型幻觉问题
- 模型识别精度问题

### 4.5 AI无参考测试

**核心思路**：利用LLM的常识和企业知识，实现无需参考答案的测试。

**技术方案：监督微调（SFT LoRA）**

**微调目标**：

- 具备更细化的任务定义
- 明确输出格式便于工程化对接
- 注入内部知识和评判标准

**微调数据结构**：

- **Instruction**：任务指令
- **Input**：测试输入
- **Output**：期望输出

**数据优化两个方向**：

**指令微调**：

- 作答风格定义
- 结构化输出要求
- 内部知识注入

**思维链微调**：

- 分析过程
- 推理步骤
- 判断原因

**数据集生成**：

- **训练集/验证集**：用于模型训练
- **测试集**：用于最终评估
- **样本比例**：正样本:负样本:混合样本 = 1:3:x

**数据合成流程**：

1. 脚本抓取线上原始样本（正样本）
2. 程序合成多个负样本
3. LLM生成Output（包含知识、结构、风格）

**当前状态**：垂直模型正在微调中。

### 4.6 AI归因归类

**传统问题**：

- 100条失败用例分析需要15分钟
- 人工分析效率低，易遗漏

**AI解决方案**：

**两步流程**：

1. **归因**：LLM总结单个失败的核心原因
2. **归类**：将类似原因的失败用例分组

**效果提升**：

- 85条失败用例被快速归为3组
- 标明核心原因
- 100条用例分析从15分钟降至1分钟

**技术实现**：

**预处理阶段**（程序）：

- 失败图片切片：解决多模态LLM在大图下的幻觉
- 步骤拆分：解决性能问题

**分析阶段**（多Agent并发）：

- 图片归因Agent：分析图片切片
- 步骤归因Agent：分析执行步骤
- 总结Agent：合并总结原因
- 归类Agent：对相似用例分组

**数据指标**：

- 线上用例100%覆盖AI归因归类
- 归因准确率：85%

### 4.7 AI用例修复

**实现场景**：

- 像素差异率波动
- 非核心元素变化

**工作模式**：

- AI对可修复场景给出建议
- 人工确认无误后一键修复

**当前准确率**：60%

### 4.8 AI Coding与AI Test的结合

**未来计划**：串联两大体系

**工作流程**：

1. Coding Agent生成测试目标
2. 交由Test Agent召回和生成用例
3. 完成自动化测试
4. 反馈结果到Coding Agent

## 五、Agent评测体系：从炫酷到生产

### 5.1 为什么需要评测

**常见现象**：

- 看别人的Agent很炫酷
- 自己手搓Agent效果不错
- 发布生产后各种问题
- 最终还是转人工

**根本原因**：Agent与传统软件的差异

**传统软件测试**：

- 目标固定、标准化、可复现
- 通过功能点完成度判断
- 测试通过即可发布

**Agent评测**：

- 具有不确定性、开放性（用户提问多样）、多样化（模型回答不确定）
- 需要多维度指标评估
- 评测指标作为发布依据

**核心认知**：Agent开发完并不意味着达到生产发布标准，应该用评测指标作为依据。

### 5.2 评测集构造

**评测集类型**：

- **有参考**：有标准答案的测试场景
- **无参考**：开放式问题，无标准答案
- **参考资料**：需要参考特定文档或知识

**构造方法**：

- **人工标注**：高质量但成本高
- **LLM泛化**：基于种子集扩展
- **线上采样**：真实用户场景

**全新Agent的评测集构建**：

1. 人工标注50-100条种子集
2. LLM进行泛化扩展
3. 线上运行后持续采样

**评测集分类**：

- **种子集**：初始标注的核心用例
- **Badcase集**：来源于用户反馈的失败案例
- **扩展集**：LLM泛化生成的扩展用例
- **对抗集**：专门设计的边界案例
- **场景集**：由业务场景决定的特定用例

### 5.3 评测器设计

**评测类型**：

- **人工评测**：初期标准不确定时使用
- **自动化评测**：将人工评测维度沉淀为自动化

**自动化评测器优势**：

- 判断尺度统一
- 主观偏差少
- 可规模化执行

**评测器分类**：

- **托管**：平台自带，快速启动
- **自定义**：根据业务编写Prompt的裁判模型
- **外部**：外部平台/服务提供

**评测器实现方式**：

- LLM作为裁判
- 特定算法验证（BLEU、CLIPScore等）
- 业务逻辑验证

**评测器设计要素**：

1. 评估步骤定义
2. 打分标准明确
3. 推理指令清晰
4. 少样本提示（正例/反例）
5. 边界案例覆盖
6. 基于业务的判断要点（合规、医疗等）

**打分规则**：

- 采用0-5分制归一化
- 兼顾可解释性和区分度
- 便于不同维度横向对比

**提升技巧**：

- 为裁判模型添加CoT（Chain of Thought）
- 提升可解释性和一致性
- 便于后续人工分析和优化

### 5.4 评测指标设定

**明确评测主体**：

- 基础模型版本
- 提示词版本
- 工具版本
- 召回的记忆版本

**四类指标**：

**效果指标**：

- 任务完成率
- 准确率
- 召回率

**技术指标**：

- 响应时间
- Token消耗
- 成功率

**用户指标**：

- 用户满意度
- 使用频率
- 留存率

**业务指标**：

- 成本节约
- 效率提升
- 质量改进

**裁判模型自身评测**：

- 与人工标注的一致性
- 不同裁判模型之间的一致性
- 打分的稳定性

### 5.5 反馈系统

**两种反馈方式**：

**显式反馈**：

- 点赞/点踩
- 评分
- 评论

**隐式反馈**：

- 使用时长
- 重试次数
- 放弃率

**提升反馈率的方法**：

- 预设标签减少用户行动成本
- 预设高评分增加反馈意愿（人们更倾向吐槽）

**反馈的价值**：发现Agent在实际应用中遇到的边界场景和潜在问题。

### 5.6 完整评测体系

**评测流程**：

1. 构建评测集（种子集→扩展集→场景集）
2. 设计评测器（裁判模型+算法验证）
3. 设定评测指标（效果+技术+用户+业务）
4. 执行评测任务
5. 收集用户反馈
6. 分析优化迭代

**发布要求**：所有Agent发布前都需要经过评测验证。

## 六、AI落地的关键经验

### 6.1 AI与程序的关系

**四种模块类型**：

- 程序计算节点
- 单一LLM节点
- Workflow
- Agentic

**关键认知**：

- 四者之间**没有绝对的优劣**
- 需要根据业务场景进行多选和组合
- 取决于对确定性、稳定性、创造性、自主性的侧重

**LLM与程序的分工**：

- **LLM更适合**：理解、规划、生成类任务
- **程序更适合**：确定性、稳定性、高性能任务

**重要原则**：通过程序规避LLM的弱项是必要的，避免过度追求AI含量。

### 6.2 AI与人的关系

**警惕两个陷阱**：

**陷阱一：过度依赖AI**

- 现象：盲目相信AI输出
- 风险：质量问题、安全风险
- 对策：保持必要的人工监督

**陷阱二：低估AI能力**

- 现象：不愿尝试AI方案
- 风险：错失效率提升机会
- 对策：积极探索AI应用场景

### 6.3 AI落地十大经验

1. **充分借助行业能力**：将通识的交给行业，集中资源做私有部分
2. **循序渐进落地**：从AI辅助→AI增强→AI驱动
3. **小步快跑验证**：从小需求开始，快速验证和迭代
4. **重视基础建设**：知识库、评测体系是关键
5. **合理人工监督**：HITL是必要的质量保障
6. **积累长期记忆**：将经验沉淀为可复用的知识
7. **建立评测体系**：用数据指标而非主观感受
8. **注重安全合规**：代码脱敏、访问控制不可忽视
9. **控制成本预期**：Token成本、算力成本需要平衡
10. **合理设定期望**：AI不是万能的，人做不了的它也做不了

## 七、对AI Native团队建设的启示

### 7.1 从技术架构视角

**分层架构设计**：

- **基础层**：选择行业成熟的基础模型和Agent框架
- **能力层**：构建代码理解、知识管理、长期记忆等核心能力
- **应用层**：开发面向具体场景的专业Agent
- **监督层**：建立完善的人工监督和评测体系

**关键设施建设**：

- 企业知识库：高质量的业务知识和技术知识
- 代码理解能力：RAG + AST + 文本搜索的混合方案
- 云端沙箱：标准化、可扩展的执行环境
- 多Agent协同：专业化分工，差异化选型

### 7.2 从组织能力视角

**专业团队分工**：

- **知识工程团队**：负责知识库建设与维护
- **算法团队**：提供模型微调、评测等AI基础能力
- **平台团队**：构建Agent开发、部署、监控平台
- **业务团队**：开发面向具体场景的Agent应用

**能力建设重点**：

- Prompt Engineering能力
- Agent设计与调试能力
- 评测体系设计能力
- 知识管理能力

### 7.3 从实施路径视角

**阶段一：基础设施准备**（1-2个月）

- 选型基础模型和Agent框架
- 搭建开发和部署平台
- 建立初步的知识库

**阶段二：小规模验证**（2-3个月）

- 选择单职能单仓库的日常需求
- 开发初版Agent
- 建立评测体系
- 积累经验和数据

**阶段三：规模化推广**（3-6个月）

- 向多职能多仓库需求扩展
- 完善监督和评测机制
- 优化成本和性能
- 沉淀最佳实践

**阶段四：持续演进**（持续进行）

- 向项目级需求挑战
- 探索AI驱动模式
- 与其他AI能力整合（如AI Test）
- 建立AI Native文化

### 7.4 从价值衡量视角

**短期价值指标**：

- 需求交付数量（已交付近百个）
- 综合提效比例（30%）
- 单需求成本（<100元）
- 用户满意度

**长期价值指标**：

- 知识沉淀规模（长期记忆条数、知识库覆盖度）
- 能力泛化程度（单职能→多职能、日常需求→项目级）
- 开发者成长（跨域编程能力、AI协作能力）
- 文化转型（从传统开发到AI Native开发）

### 7.5 从风险控制视角

**技术风险**：

- AI不确定性：通过评测体系和人工监督控制
- 代码安全：所有代码片段进行脱敏处理
- 依赖风险：避免过度依赖单一供应商

**管理风险**：

- 期望管理：明确AI能做什么、不能做什么
- 成本控制：Token费用、算力成本的预算管理
- 质量保障：不能因追求速度而降低质量标准

**组织风险**：

- 技能转型：开发者需要学习新的协作模式
- 角色变化：从直接产出到对算力的设计与编排
- 文化冲突：传统开发模式与AI协作模式的冲突

### 7.6 有赞实践的核心洞察

**洞察一：马拉火车终将被超越**

- AI能力正在快速提升
- 人力到算力的转型不可逆
- 但转型需要循序渐进

**洞察二：基础设施决定上限**

- 知识库质量决定AI效果
- 评测体系决定可信度
- 长期记忆决定成长性

**洞察三：人机协作是现阶段最优解**

- AI增强阶段是当前主战场
- 人的监督和决策不可或缺
- 目标是让人做更有价值的工作

**洞察四：数据驱动持续改进**

- 每次交互都是学习机会
- 评测数据指导优化方向
- 用户反馈是宝贵财富

## 八、结语

有赞的AI研发全流程落地实践，展示了从"炫酷"到"生产落地"的完整路径。其核心价值不在于某个技术点的创新，而在于系统化地解决了企业级AI应用的四大挑战：复杂度、规范性、协同性、质量要求。

通过AI Coding和AI Test的深度实践，有赞证明了：**在合适的场景、用正确的方法，AI可以真正提升研发效能，而不仅仅是炫技。**

对于正在探索AI Native团队建设的组织，有赞的经验提供了宝贵的参考：从小做起、循序渐进、数据驱动、持续改进。AI的落地不是一蹴而就的革命，而是一场需要耐心和智慧的演进。
